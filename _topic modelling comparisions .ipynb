{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clearcut topics\n",
      "[('we love bergers', 1000), ('we hate sandwiches', 1000)]\n",
      "\n",
      "unbalanced topics\n",
      "[('we hate sandwiches', 1000), ('we love bergers', 10)]\n",
      "\n",
      "semantic topics\n",
      "[('we love bergers', 1000), ('we hate bergers', 1000), ('we love sandwiches', 1000), ('we hate sandwiches', 1000)]\n",
      "\n",
      "noisy topics\n",
      "[('we love bXrgers', 4), ('we hate sandwXches', 4), ('we love bergXrs', 3), ('we love bergeXs', 3), ('we hate saXdwiches', 3), ('we love beXgers', 2), ('we love Xergers', 2), ('we hate sandXiches', 2), ('we hate Xandwiches', 2), ('we hate sandwicXes', 2), ('we love berXers', 1), ('we hate sandwichXs', 1), ('we hate sandwicheX', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "def generate_clearcut_topics():\n",
    "    ## for demostration purpose, don't take it personally : )\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [1000, 1000])\n",
    "\n",
    "def generate_unbalanced_topics():\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [10, 1000])\n",
    "\n",
    "def generate_semantic_context_topics():\n",
    "    return np.repeat([\"we love bergers\"\n",
    "                      , \"we hate bergers\"\n",
    "                      , \"we love sandwiches\"\n",
    "                      , \"we hate sandwiches\"], 1000)\n",
    "\n",
    "def generate_noisy_topics():\n",
    "    def _random_typos(word, n):\n",
    "        typo_index = np.random.randint(0, len(word), n)\n",
    "        return [word[:i]+\"X\"+word[i+1:] for i in typo_index]\n",
    "    t1 = [\"we love %s\" % w for w in _random_typos(\"bergers\", 15)]\n",
    "    t2 = [\"we hate %s\" % w for w in _random_typos(\"sandwiches\", 15)]\n",
    "    return np.r_[t1, t2]\n",
    "\n",
    "sample_texts = {\n",
    "     \"clearcut topics\": generate_clearcut_topics()\n",
    "    , \"unbalanced topics\": generate_unbalanced_topics()\n",
    "    , \"semantic topics\": generate_semantic_context_topics()\n",
    "    , \"noisy topics\": generate_noisy_topics()\n",
    "}\n",
    "from collections import Counter\n",
    "for desc, texts in sample_texts.items():\n",
    "    print (desc)\n",
    "    print (Counter(texts).most_common())\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
